"""
LLM ç›¸å…³æ•°æ®æ¨¡å‹å®šä¹‰
"""

from enum import Enum
from typing import Any, Dict, List, Optional, Union
from pydantic import BaseModel, Field


class LLMProvider(str, Enum):
    """LLM æä¾›å•†æšä¸¾"""
    OPENAI = "openai"
    DEEPSEEK = "deepseek"
    DASHSCOPE = "dashscope"  # é€šä¹‰åƒé—®
    ZHIPU = "zhipu"          # æ™ºè°±AI
    SILICONFLOW = "siliconflow"  # ç¡…åŸºæµåŠ¨
    GOOGLE = "google"
    ANTHROPIC = "anthropic"
    OLLAMA = "ollama"
    OPENROUTER = "openrouter"


class MessageRole(str, Enum):
    """æ¶ˆæ¯è§’è‰²"""
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"
    TOOL = "tool"


class Message(BaseModel):
    """ç»Ÿä¸€æ¶ˆæ¯æ ¼å¼"""
    role: MessageRole
    content: Optional[str] = None
    name: Optional[str] = None
    tool_calls: Optional[List["ToolCall"]] = None
    tool_call_id: Optional[str] = None
    
    class Config:
        use_enum_values = True


class ToolCall(BaseModel):
    """å·¥å…·è°ƒç”¨å®šä¹‰"""
    id: str
    name: str
    arguments: Dict[str, Any] = Field(default_factory=dict)
    
    @classmethod
    def from_openai_format(cls, tool_call: dict) -> "ToolCall":
        """ä» OpenAI æ ¼å¼è½¬æ¢"""
        import json
        return cls(
            id=tool_call.get("id", ""),
            name=tool_call.get("function", {}).get("name", ""),
            arguments=json.loads(tool_call.get("function", {}).get("arguments", "{}"))
        )
    
    @classmethod
    def from_google_format(cls, tool_call: dict) -> "ToolCall":
        """ä» Google æ ¼å¼è½¬æ¢"""
        return cls(
            id=tool_call.get("id", f"call_{hash(str(tool_call))}"),
            name=tool_call.get("name", ""),
            arguments=tool_call.get("args", {})
        )


class ToolResult(BaseModel):
    """å·¥å…·æ‰§è¡Œç»“æœ"""
    tool_call_id: str
    name: str
    content: str
    is_error: bool = False


class LLMConfig(BaseModel):
    """LLM é…ç½®"""
    provider: LLMProvider
    model: str
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    temperature: float = 0.2  # è‚¡ç¥¨åˆ†ææ¨èå€¼ï¼š0.2-0.3ï¼ˆå¿«é€Ÿåˆ†æï¼‰ï¼Œ0.1-0.2ï¼ˆæ·±åº¦åˆ†æï¼‰
    max_tokens: Optional[int] = None
    timeout: int = 60
    
    # é«˜çº§é…ç½®
    top_p: Optional[float] = None
    frequency_penalty: Optional[float] = None
    presence_penalty: Optional[float] = None
    
    # å·¥å…·è°ƒç”¨é…ç½®
    tool_choice: Optional[str] = None  # "auto", "none", "required"
    parallel_tool_calls: bool = True
    
    class Config:
        use_enum_values = True
    
    @classmethod
    def from_env(cls, provider: LLMProvider) -> "LLMConfig":
        """ä»ç¯å¢ƒå˜é‡åˆ›å»ºé…ç½®"""
        import os
        import logging
        
        logger = logging.getLogger(__name__)
        
        config_map = {
            LLMProvider.OPENAI: {
                "api_key": os.getenv("OPENAI_API_KEY"),
                "base_url": os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1"),
                "model": os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            },
            LLMProvider.DEEPSEEK: {
                "api_key": os.getenv("DEEPSEEK_API_KEY"),
                "base_url": "https://api.deepseek.com",
                "model": os.getenv("DEEPSEEK_MODEL", "deepseek-chat"),
            },
            LLMProvider.DASHSCOPE: {
                "api_key": os.getenv("DASHSCOPE_API_KEY"),
                "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
                "model": os.getenv("DASHSCOPE_MODEL", "qwen-plus"),
            },
            LLMProvider.ZHIPU: {
                "api_key": os.getenv("ZHIPU_API_KEY"),
                "base_url": "https://open.bigmodel.cn/api/paas/v4",
                "model": os.getenv("ZHIPU_MODEL", "glm-4"),
            },
            LLMProvider.GOOGLE: {
                "api_key": os.getenv("GOOGLE_API_KEY"),
                "model": os.getenv("GOOGLE_MODEL", "gemini-2.0-flash"),
            },
            LLMProvider.ANTHROPIC: {
                "api_key": os.getenv("ANTHROPIC_API_KEY"),
                "model": os.getenv("ANTHROPIC_MODEL", "claude-sonnet-4-20250514"),
            },
        }
        
        if provider not in config_map:
            raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}")
        
        config_data = config_map[provider]
        api_key = config_data.get("api_key")
        
        # ğŸ”¥ æ£€æŸ¥ API key æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è®°å½•è­¦å‘Š
        if not api_key:
            env_key_map = {
                LLMProvider.OPENAI: "OPENAI_API_KEY",
                LLMProvider.DEEPSEEK: "DEEPSEEK_API_KEY",
                LLMProvider.DASHSCOPE: "DASHSCOPE_API_KEY",
                LLMProvider.ZHIPU: "ZHIPU_API_KEY",
                LLMProvider.GOOGLE: "GOOGLE_API_KEY",
                LLMProvider.ANTHROPIC: "ANTHROPIC_API_KEY",
            }
            expected_env_key = env_key_map.get(provider, "API_KEY")
            logger.warning(
                f"âš ï¸ ç¯å¢ƒå˜é‡ {expected_env_key} æœªè®¾ç½®ï¼ŒLLM å®¢æˆ·ç«¯å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œã€‚"
                f"Provider: {provider.value}, Model: {config_data.get('model', 'unknown')}"
            )
            logger.warning(
                f"ğŸ’¡ è¯·ç¡®ä¿å·²è°ƒç”¨ bridge_config_to_env() æˆ–æ‰‹åŠ¨è®¾ç½®ç¯å¢ƒå˜é‡ {expected_env_key}"
            )
        
        return cls(provider=provider, **config_data)


class LLMResponse(BaseModel):
    """LLM å“åº”"""
    content: Optional[str] = None
    tool_calls: List[ToolCall] = Field(default_factory=list)
    finish_reason: Optional[str] = None
    
    # å…ƒæ•°æ®
    model: Optional[str] = None
    provider: Optional[LLMProvider] = None
    usage: Optional[Dict[str, int]] = None
    
    @property
    def has_tool_calls(self) -> bool:
        """æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨"""
        return len(self.tool_calls) > 0
    
    def to_message(self) -> Message:
        """è½¬æ¢ä¸ºæ¶ˆæ¯æ ¼å¼"""
        return Message(
            role=MessageRole.ASSISTANT,
            content=self.content,
            tool_calls=self.tool_calls if self.tool_calls else None
        )


# æ›´æ–°å‰å‘å¼•ç”¨
Message.model_rebuild()

