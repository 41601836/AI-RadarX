#!/usr/bin/env python3
"""
é›†æˆç¼“å­˜ç®¡ç†å™¨
ç»“åˆåŸæœ‰ç¼“å­˜ç³»ç»Ÿå’Œæ–°çš„è‡ªé€‚åº”æ•°æ®åº“æ”¯æŒ
æä¾›å‘åå…¼å®¹çš„æ¥å£
"""

import os
import logging
from pathlib import Path
from typing import Any, Dict, Optional, Union
import pandas as pd

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import setup_dataflow_logging

# å¯¼å…¥åŸæœ‰ç¼“å­˜ç³»ç»Ÿ
from .file_cache import StockDataCache

# å¯¼å…¥è‡ªé€‚åº”ç¼“å­˜ç³»ç»Ÿ
try:
    from .adaptive import AdaptiveCacheSystem
    from tradingagents.config.database_manager import get_database_manager
    ADAPTIVE_CACHE_AVAILABLE = True
except ImportError as e:
    ADAPTIVE_CACHE_AVAILABLE = False
    import logging
    logging.getLogger(__name__).debug(f"è‡ªé€‚åº”ç¼“å­˜ä¸å¯ç”¨: {e}")

class IntegratedCacheManager:
    """é›†æˆç¼“å­˜ç®¡ç†å™¨ - æ™ºèƒ½é€‰æ‹©ç¼“å­˜ç­–ç•¥"""
    
    def __init__(self, cache_dir: str = None):
        self.logger = setup_dataflow_logging()
        
        # åˆå§‹åŒ–åŸæœ‰ç¼“å­˜ç³»ç»Ÿï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰
        self.legacy_cache = StockDataCache(cache_dir)
        
        # å°è¯•åˆå§‹åŒ–è‡ªé€‚åº”ç¼“å­˜ç³»ç»Ÿ
        self.adaptive_cache = None
        self.use_adaptive = False
        
        if ADAPTIVE_CACHE_AVAILABLE:
            try:
                self.adaptive_cache = AdaptiveCacheSystem(cache_dir)
                self.db_manager = get_database_manager()
                self.use_adaptive = True
                self.logger.info("âœ… è‡ªé€‚åº”ç¼“å­˜ç³»ç»Ÿå·²å¯ç”¨")
            except Exception as e:
                self.logger.warning(f"è‡ªé€‚åº”ç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿç¼“å­˜: {e}")
                self.use_adaptive = False
        else:
            self.logger.info("è‡ªé€‚åº”ç¼“å­˜ç³»ç»Ÿä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–‡ä»¶ç¼“å­˜")
        
        # æ˜¾ç¤ºå½“å‰é…ç½®
        self._log_cache_status()
    
    def _log_cache_status(self):
        """è®°å½•ç¼“å­˜çŠ¶æ€"""
        if self.use_adaptive:
            backend = self.adaptive_cache.primary_backend
            mongodb_available = self.db_manager.is_mongodb_available()
            redis_available = self.db_manager.is_redis_available()
            
            self.logger.info(f"ğŸ“Š ç¼“å­˜é…ç½®:")
            self.logger.info(f"  ä¸»è¦åç«¯: {backend}")
            self.logger.info(f"  MongoDB: {'âœ… å¯ç”¨' if mongodb_available else 'âŒ ä¸å¯ç”¨'}")
            self.logger.info(f"  Redis: {'âœ… å¯ç”¨' if redis_available else 'âŒ ä¸å¯ç”¨'}")
            self.logger.info(f"  é™çº§æ”¯æŒ: {'âœ… å¯ç”¨' if self.adaptive_cache.fallback_enabled else 'âŒ ç¦ç”¨'}")
        else:
            self.logger.info("ğŸ“ ä½¿ç”¨ä¼ ç»Ÿæ–‡ä»¶ç¼“å­˜ç³»ç»Ÿ")
    
    def save_stock_data(self, symbol: str, data: Any, start_date: str = None, 
                       end_date: str = None, data_source: str = "default") -> str:
        """
        ä¿å­˜è‚¡ç¥¨æ•°æ®åˆ°ç¼“å­˜
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data: è‚¡ç¥¨æ•°æ®
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            data_source: æ•°æ®æº
            
        Returns:
            ç¼“å­˜é”®
        """
        if self.use_adaptive:
            # ä½¿ç”¨è‡ªé€‚åº”ç¼“å­˜ç³»ç»Ÿ
            return self.adaptive_cache.save_data(
                symbol=symbol,
                data=data,
                start_date=start_date or "",
                end_date=end_date or "",
                data_source=data_source,
                data_type="stock_data"
            )
        else:
            # ä½¿ç”¨ä¼ ç»Ÿç¼“å­˜ç³»ç»Ÿ
            return self.legacy_cache.save_stock_data(
                symbol=symbol,
                data=data,
                start_date=start_date,
                end_date=end_date,
                data_source=data_source
            )
    
    def load_stock_data(self, cache_key: str) -> Optional[Any]:
        """
        ä»ç¼“å­˜åŠ è½½è‚¡ç¥¨æ•°æ®
        
        Args:
            cache_key: ç¼“å­˜é”®
            
        Returns:
            è‚¡ç¥¨æ•°æ®æˆ–None
        """
        if self.use_adaptive:
            # ä½¿ç”¨è‡ªé€‚åº”ç¼“å­˜ç³»ç»Ÿ
            return self.adaptive_cache.load_data(cache_key)
        else:
            # ä½¿ç”¨ä¼ ç»Ÿç¼“å­˜ç³»ç»Ÿ
            return self.legacy_cache.load_stock_data(cache_key)
    
    def find_cached_stock_data(self, symbol: str, start_date: str = None, 
                              end_date: str = None, data_source: str = "default") -> Optional[str]:
        """
        æŸ¥æ‰¾ç¼“å­˜çš„è‚¡ç¥¨æ•°æ®
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            data_source: æ•°æ®æº
            
        Returns:
            ç¼“å­˜é”®æˆ–None
        """
        if self.use_adaptive:
            # ä½¿ç”¨è‡ªé€‚åº”ç¼“å­˜ç³»ç»Ÿ
            return self.adaptive_cache.find_cached_data(
                symbol=symbol,
                start_date=start_date or "",
                end_date=end_date or "",
                data_source=data_source,
                data_type="stock_data"
            )
        else:
            # ä½¿ç”¨ä¼ ç»Ÿç¼“å­˜ç³»ç»Ÿ
            return self.legacy_cache.find_cached_stock_data(
                symbol=symbol,
                start_date=start_date,
                end_date=end_date,
                data_source=data_source
            )
    
    def save_news_data(self, symbol: str, data: Any, data_source: str = "default") -> str:
        """ä¿å­˜æ–°é—»æ•°æ®"""
        if self.use_adaptive:
            return self.adaptive_cache.save_data(
                symbol=symbol,
                data=data,
                data_source=data_source,
                data_type="news_data"
            )
        else:
            return self.legacy_cache.save_news_data(symbol, data, data_source)
    
    def load_news_data(self, cache_key: str) -> Optional[Any]:
        """åŠ è½½æ–°é—»æ•°æ®"""
        if self.use_adaptive:
            return self.adaptive_cache.load_data(cache_key)
        else:
            return self.legacy_cache.load_news_data(cache_key)
    
    def save_fundamentals_data(self, symbol: str, data: Any, data_source: str = "default") -> str:
        """ä¿å­˜åŸºæœ¬é¢æ•°æ®"""
        if self.use_adaptive:
            return self.adaptive_cache.save_data(
                symbol=symbol,
                data=data,
                data_source=data_source,
                data_type="fundamentals_data"
            )
        else:
            return self.legacy_cache.save_fundamentals_data(symbol, data, data_source)
    
    def load_fundamentals_data(self, cache_key: str) -> Optional[Any]:
        """åŠ è½½åŸºæœ¬é¢æ•°æ®"""
        if self.use_adaptive:
            return self.adaptive_cache.load_data(cache_key)
        else:
            return self.legacy_cache.load_fundamentals_data(cache_key)

    def find_cached_fundamentals_data(self, symbol: str, data_source: str = None,
                                     max_age_hours: int = None) -> Optional[str]:
        """
        æŸ¥æ‰¾åŒ¹é…çš„åŸºæœ¬é¢ç¼“å­˜æ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data_source: æ•°æ®æºï¼ˆå¦‚ "openai", "finnhub"ï¼‰
            max_age_hours: æœ€å¤§ç¼“å­˜æ—¶é—´ï¼ˆå°æ—¶ï¼‰ï¼ŒNoneæ—¶ä½¿ç”¨æ™ºèƒ½é…ç½®

        Returns:
            cache_key: å¦‚æœæ‰¾åˆ°æœ‰æ•ˆç¼“å­˜åˆ™è¿”å›ç¼“å­˜é”®ï¼Œå¦åˆ™è¿”å›None
        """
        if self.use_adaptive:
            # è‡ªé€‚åº”ç¼“å­˜æš‚ä¸æ”¯æŒæŸ¥æ‰¾åŠŸèƒ½ï¼Œé™çº§åˆ°æ–‡ä»¶ç¼“å­˜
            return self.legacy_cache.find_cached_fundamentals_data(symbol, data_source, max_age_hours)
        else:
            return self.legacy_cache.find_cached_fundamentals_data(symbol, data_source, max_age_hours)

    def is_fundamentals_cache_valid(self, symbol: str, data_source: str = None,
                                   max_age_hours: int = None) -> bool:
        """
        æ£€æŸ¥åŸºæœ¬é¢ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data_source: æ•°æ®æº
            max_age_hours: æœ€å¤§ç¼“å­˜æ—¶é—´ï¼ˆå°æ—¶ï¼‰

        Returns:
            bool: ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        """
        cache_key = self.find_cached_fundamentals_data(symbol, data_source, max_age_hours)
        return cache_key is not None

    # ========== åˆ†ææŠ¥å‘Šç¼“å­˜ ==========

    def save_analysis_report(self, report_type: str, report_data: str,
                            symbol: str = None, trade_date: str = None,
                            data_source: str = "llm") -> str:
        """
        ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°ç¼“å­˜

        Args:
            report_type: æŠ¥å‘Šç±»å‹ï¼ˆå¦‚ "sector_report", "index_report"ï¼‰
            report_data: æŠ¥å‘Šå†…å®¹
            symbol: è‚¡ç¥¨ä»£ç ï¼ˆå¤§ç›˜åˆ†æå¯ä»¥ä¸ºç©ºï¼‰
            trade_date: äº¤æ˜“æ—¥æœŸ
            data_source: æ•°æ®æºï¼ˆé»˜è®¤ä¸º "llm"ï¼‰

        Returns:
            cache_key: ç¼“å­˜é”®
        """
        # æ„å»ºç¼“å­˜é”®ï¼šreport_type_symbol_trade_date_data_source
        key_parts = [report_type]
        if symbol:
            key_parts.append(symbol)
        if trade_date:
            # ç¡®ä¿ trade_date æ˜¯å­—ç¬¦ä¸²æ ¼å¼
            if hasattr(trade_date, 'strftime'):
                # datetime å¯¹è±¡
                trade_date_str = trade_date.strftime("%Y%m%d")
            else:
                # å­—ç¬¦ä¸²ï¼Œç§»é™¤åˆ†éš”ç¬¦
                trade_date_str = str(trade_date).split()[0].replace("-", "")
            key_parts.append(trade_date_str)
        key_parts.append(data_source)
        cache_key = "_".join(key_parts)

        # ç¡®ä¿æ—¥æœŸæ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼ˆç”¨äº adaptive_cacheï¼‰
        if trade_date:
            if hasattr(trade_date, 'strftime'):
                trade_date_str_for_cache = trade_date.strftime("%Y-%m-%d")
            else:
                trade_date_str_for_cache = str(trade_date).split()[0]
        else:
            trade_date_str_for_cache = ""

        # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šè®°å½•ä¿å­˜å‚æ•°
        self.logger.debug(f"ğŸ’¾ [IntegratedCache] save_analysis_report: report_type={report_type}, "
                          f"symbol={symbol}, trade_date={trade_date}, use_adaptive={self.use_adaptive}")

        if self.use_adaptive:
            return self.adaptive_cache.save_data(
                symbol=symbol or "market",
                data=report_data,
                start_date=trade_date_str_for_cache,
                end_date=trade_date_str_for_cache,
                data_source=data_source,
                data_type=report_type
            )
        else:
            # ä½¿ç”¨æ–‡ä»¶ç¼“å­˜çš„é€šç”¨ä¿å­˜æ–¹æ³•
            return self.legacy_cache.save_fundamentals_data(
                symbol=cache_key,
                fundamentals_data=report_data,
                data_source=data_source
            )

    def load_analysis_report(self, cache_key: str) -> Optional[str]:
        """
        ä»ç¼“å­˜åŠ è½½åˆ†ææŠ¥å‘Š

        Args:
            cache_key: ç¼“å­˜é”®

        Returns:
            æŠ¥å‘Šå†…å®¹ï¼Œå¦‚æœç¼“å­˜ä¸å­˜åœ¨æˆ–å¤±æ•ˆåˆ™è¿”å› None
        """
        if self.use_adaptive:
            return self.adaptive_cache.load_data(cache_key)
        else:
            return self.legacy_cache.load_fundamentals_data(cache_key)

    def find_cached_analysis_report(self, report_type: str, symbol: str = None,
                                   trade_date: str = None, data_source: str = "llm",
                                   max_age_hours: int = 12) -> Optional[str]:
        """
        æŸ¥æ‰¾ç¼“å­˜çš„åˆ†ææŠ¥å‘Š

        Args:
            report_type: æŠ¥å‘Šç±»å‹ï¼ˆå¦‚ "sector_report", "index_report"ï¼‰
            symbol: è‚¡ç¥¨ä»£ç ï¼ˆå¤§ç›˜åˆ†æå¯ä»¥ä¸ºç©ºï¼‰
            trade_date: äº¤æ˜“æ—¥æœŸ
            data_source: æ•°æ®æº
            max_age_hours: æœ€å¤§ç¼“å­˜æ—¶é—´ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤12å°æ—¶

        Returns:
            cache_key: å¦‚æœæ‰¾åˆ°æœ‰æ•ˆç¼“å­˜åˆ™è¿”å›ç¼“å­˜é”®ï¼Œå¦åˆ™è¿”å› None
        """
        if self.use_adaptive:
            # ä½¿ç”¨è‡ªé€‚åº”ç¼“å­˜çš„æŸ¥æ‰¾æ–¹æ³•
            # å‚æ•°åŒ¹é… save_analysis_report çš„ä¿å­˜æ–¹å¼
            cache_key = self.adaptive_cache.find_cached_data(
                symbol=symbol or "market",
                start_date=trade_date or "",
                end_date=trade_date or "",
                data_source=data_source,
                data_type=report_type  # report_type ä½œä¸º data_type
            )
            if cache_key:
                self.logger.info(f"ğŸ¯ æ‰¾åˆ°{report_type}ç¼“å­˜: {symbol or 'market'} @ {trade_date}")
                return cache_key
            else:
                self.logger.info(f"ğŸ“­ æœªæ‰¾åˆ°{report_type}ç¼“å­˜: {symbol or 'market'} @ {trade_date}")
                return None
        else:
            # å¯¹äºæ–‡ä»¶ç¼“å­˜ï¼Œæ„å»ºç¼“å­˜é”®å¹¶æŸ¥æ‰¾
            key_parts = [report_type]
            if symbol:
                key_parts.append(symbol)
            if trade_date:
                key_parts.append(trade_date.replace("-", ""))
            key_parts.append(data_source)
            cache_key = "_".join(key_parts)
            return self.legacy_cache.find_cached_fundamentals_data(cache_key, data_source, max_age_hours)

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if self.use_adaptive:
            # è·å–è‡ªé€‚åº”ç¼“å­˜ç»Ÿè®¡ï¼ˆå·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼‰
            stats = self.adaptive_cache.get_cache_stats()

            # æ·»åŠ ç¼“å­˜ç³»ç»Ÿä¿¡æ¯
            stats['cache_system'] = 'adaptive'

            # ç¡®ä¿åç«¯ä¿¡æ¯å­˜åœ¨
            if 'backend_info' not in stats:
                stats['backend_info'] = {}

            stats['backend_info']['database_available'] = self.db_manager.is_database_available()
            stats['backend_info']['mongodb_available'] = self.db_manager.is_mongodb_available()
            stats['backend_info']['redis_available'] = self.db_manager.is_redis_available()

            return stats
        else:
            # è¿”å›ä¼ ç»Ÿç¼“å­˜ç»Ÿè®¡ï¼ˆå·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼‰
            stats = self.legacy_cache.get_cache_stats()

            # æ·»åŠ ç¼“å­˜ç³»ç»Ÿä¿¡æ¯
            stats['cache_system'] = 'legacy'

            # ç¡®ä¿åç«¯ä¿¡æ¯å­˜åœ¨
            if 'backend_info' not in stats:
                stats['backend_info'] = {}

            stats['backend_info']['database_available'] = False
            stats['backend_info']['mongodb_available'] = False
            stats['backend_info']['redis_available'] = False

            return stats
    
    def clear_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        if self.use_adaptive:
            self.adaptive_cache.clear_expired_cache()

        # æ€»æ˜¯æ¸…ç†ä¼ ç»Ÿç¼“å­˜
        self.legacy_cache.clear_expired_cache()

    def clear_old_cache(self, max_age_days: int = 7):
        """
        æ¸…ç†è¿‡æœŸç¼“å­˜ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰

        Args:
            max_age_days: æ¸…ç†å¤šå°‘å¤©å‰çš„ç¼“å­˜ï¼Œ0è¡¨ç¤ºæ¸…ç†æ‰€æœ‰ç¼“å­˜

        Returns:
            æ¸…ç†çš„è®°å½•æ•°
        """
        cleared_count = 0

        # 1. æ¸…ç† Redis ç¼“å­˜
        if self.use_adaptive and self.db_manager.is_redis_available():
            try:
                redis_client = self.db_manager.get_redis_client()
                if max_age_days == 0:
                    # æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
                    redis_client.flushdb()
                    self.logger.info(f"ğŸ§¹ Redis ç¼“å­˜å·²å…¨éƒ¨æ¸…ç©º")
                else:
                    # Redis ä¼šè‡ªåŠ¨è¿‡æœŸï¼Œè¿™é‡Œåªè®°å½•æ—¥å¿—
                    self.logger.info(f"ğŸ§¹ Redis ç¼“å­˜ä¼šè‡ªåŠ¨è¿‡æœŸï¼ˆTTLæœºåˆ¶ï¼‰")
            except Exception as e:
                self.logger.error(f"âš ï¸ Redis ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")

        # 2. æ¸…ç† MongoDB ç¼“å­˜
        if self.use_adaptive and self.db_manager.is_mongodb_available():
            try:
                from datetime import datetime, timedelta
                from zoneinfo import ZoneInfo
                from tradingagents.config.runtime_settings import get_timezone_name

                mongodb_db = self.db_manager.get_mongodb_db()

                if max_age_days == 0:
                    # æ¸…ç©ºæ‰€æœ‰ç¼“å­˜é›†åˆ
                    for collection_name in ["stock_data", "news_data", "fundamentals_data"]:
                        result = mongodb_db[collection_name].delete_many({})
                        cleared_count += result.deleted_count
                        self.logger.info(f"ğŸ§¹ MongoDB {collection_name} æ¸…ç©ºäº† {result.deleted_count} æ¡è®°å½•")
                else:
                    # æ¸…ç†è¿‡æœŸæ•°æ®
                    cutoff_time = datetime.now(ZoneInfo(get_timezone_name())) - timedelta(days=max_age_days)
                    for collection_name in ["stock_data", "news_data", "fundamentals_data"]:
                        result = mongodb_db[collection_name].delete_many({"created_at": {"$lt": cutoff_time}})
                        cleared_count += result.deleted_count
                        self.logger.info(f"ğŸ§¹ MongoDB {collection_name} æ¸…ç†äº† {result.deleted_count} æ¡è®°å½•")
            except Exception as e:
                self.logger.error(f"âš ï¸ MongoDB ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")

        # 3. æ¸…ç†æ–‡ä»¶ç¼“å­˜
        try:
            file_cleared = self.legacy_cache.clear_old_cache(max_age_days)
            # æ–‡ä»¶ç¼“å­˜å¯èƒ½è¿”å› Noneï¼Œéœ€è¦å¤„ç†
            if file_cleared is not None:
                cleared_count += file_cleared
                self.logger.info(f"ğŸ§¹ æ–‡ä»¶ç¼“å­˜æ¸…ç†äº† {file_cleared} ä¸ªæ–‡ä»¶")
            else:
                self.logger.info(f"ğŸ§¹ æ–‡ä»¶ç¼“å­˜æ¸…ç†å®Œæˆï¼ˆè¿”å›å€¼ä¸ºNoneï¼‰")
        except Exception as e:
            self.logger.error(f"âš ï¸ æ–‡ä»¶ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")

        self.logger.info(f"ğŸ§¹ æ€»å…±æ¸…ç†äº† {cleared_count} æ¡ç¼“å­˜è®°å½•")
        return cleared_count
    
    def get_cache_backend_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜åç«¯ä¿¡æ¯"""
        if self.use_adaptive:
            return {
                "system": "adaptive",
                "primary_backend": self.adaptive_cache.primary_backend,
                "fallback_enabled": self.adaptive_cache.fallback_enabled,
                "mongodb_available": self.db_manager.is_mongodb_available(),
                "redis_available": self.db_manager.is_redis_available()
            }
        else:
            return {
                "system": "legacy",
                "primary_backend": "file",
                "fallback_enabled": False,
                "mongodb_available": False,
                "redis_available": False
            }
    
    def is_database_available(self) -> bool:
        """æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å¯ç”¨"""
        if self.use_adaptive:
            return self.db_manager.is_database_available()
        return False
    
    def get_performance_mode(self) -> str:
        """è·å–æ€§èƒ½æ¨¡å¼"""
        if not self.use_adaptive:
            return "åŸºç¡€æ¨¡å¼ (æ–‡ä»¶ç¼“å­˜)"
        
        mongodb_available = self.db_manager.is_mongodb_available()
        redis_available = self.db_manager.is_redis_available()
        
        if redis_available and mongodb_available:
            return "é«˜æ€§èƒ½æ¨¡å¼ (Redis + MongoDB + æ–‡ä»¶)"
        elif redis_available:
            return "å¿«é€Ÿæ¨¡å¼ (Redis + æ–‡ä»¶)"
        elif mongodb_available:
            return "æŒä¹…åŒ–æ¨¡å¼ (MongoDB + æ–‡ä»¶)"
        else:
            return "æ ‡å‡†æ¨¡å¼ (æ™ºèƒ½æ–‡ä»¶ç¼“å­˜)"


# å…¨å±€é›†æˆç¼“å­˜ç®¡ç†å™¨å®ä¾‹
_integrated_cache = None

def get_cache() -> IntegratedCacheManager:
    """è·å–å…¨å±€é›†æˆç¼“å­˜ç®¡ç†å™¨å®ä¾‹"""
    global _integrated_cache
    if _integrated_cache is None:
        _integrated_cache = IntegratedCacheManager()
    return _integrated_cache

# å‘åå…¼å®¹çš„å‡½æ•°
def get_stock_cache():
    """å‘åå…¼å®¹ï¼šè·å–è‚¡ç¥¨ç¼“å­˜"""
    return get_cache()

def create_cache_manager(cache_dir: str = None):
    """å‘åå…¼å®¹ï¼šåˆ›å»ºç¼“å­˜ç®¡ç†å™¨"""
    return IntegratedCacheManager(cache_dir)
