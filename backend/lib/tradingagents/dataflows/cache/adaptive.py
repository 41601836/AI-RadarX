#!/usr/bin/env python3
"""
è‡ªé€‚åº”ç¼“å­˜ç³»ç»Ÿ
æ ¹æ®æ•°æ®åº“å¯ç”¨æ€§è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç¼“å­˜ç­–ç•¥
"""

import os
import json
import pickle
import hashlib
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, Optional, Union
import pandas as pd

from tradingagents.config.database_manager import get_database_manager

class AdaptiveCacheSystem:
    """è‡ªé€‚åº”ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self, cache_dir: str = None):
        self.logger = logging.getLogger(__name__)

        # è·å–æ•°æ®åº“ç®¡ç†å™¨
        self.db_manager = get_database_manager()

        # è®¾ç½®ç¼“å­˜ç›®å½•
        if cache_dir is None:
            # é»˜è®¤ä½¿ç”¨ data/cache ç›®å½•
            cache_dir = "data/cache"
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # è·å–é…ç½®
        self.config = self.db_manager.get_config()
        self.cache_config = self.config["cache"]
        
        # åˆå§‹åŒ–ç¼“å­˜åç«¯
        self.primary_backend = self.cache_config["primary_backend"]
        self.fallback_enabled = self.cache_config["fallback_enabled"]
        
        self.logger.info(f"è‡ªé€‚åº”ç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ– - ä¸»è¦åç«¯: {self.primary_backend}")
    
    def _get_cache_key(self, symbol: str, start_date: str = "", end_date: str = "", 
                      data_source: str = "default", data_type: str = "stock_data") -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = f"{symbol}_{start_date}_{end_date}_{data_source}_{data_type}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def _get_ttl_seconds(self, symbol: str, data_type: str = "stock_data") -> int:
        """è·å–TTLç§’æ•°"""
        # åˆ¤æ–­å¸‚åœºç±»å‹
        if len(symbol) == 6 and symbol.isdigit():
            market = "china"
        else:
            market = "us"

        # è·å–TTLé…ç½®
        ttl_key = f"{market}_{data_type}"
        ttl_seconds = self.cache_config["ttl_settings"].get(ttl_key, 7200)

        # ç¡®ä¿è¿”å›æ•´æ•°ï¼ˆé…ç½®æ–‡ä»¶ä¸­å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼‰
        try:
            return int(ttl_seconds)
        except (ValueError, TypeError):
            self.logger.warning(f"âš ï¸ TTLé…ç½®æ— æ•ˆ: {ttl_key}={ttl_seconds}, ä½¿ç”¨é»˜è®¤å€¼7200ç§’")
            return 7200
    
    def _is_cache_valid(self, cache_time: datetime, ttl_seconds: int) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if cache_time is None:
            return False
        
        expiry_time = cache_time + timedelta(seconds=ttl_seconds)
        return datetime.now() < expiry_time
    
    def _save_to_file(self, cache_key: str, data: Any, metadata: Dict) -> bool:
        """ä¿å­˜åˆ°æ–‡ä»¶ç¼“å­˜"""
        try:
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            cache_data = {
                'data': data,
                'metadata': metadata,
                'timestamp': datetime.now(),
                'backend': 'file'
            }
            
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            
            self.logger.debug(f"æ–‡ä»¶ç¼“å­˜ä¿å­˜æˆåŠŸ: {cache_key}")
            return True
            
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def _load_from_file(self, cache_key: str) -> Optional[Dict]:
        """ä»æ–‡ä»¶ç¼“å­˜åŠ è½½"""
        try:
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            if not cache_file.exists():
                return None
            
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            self.logger.debug(f"æ–‡ä»¶ç¼“å­˜åŠ è½½æˆåŠŸ: {cache_key}")
            return cache_data
            
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            return None
    
    def _save_to_redis(self, cache_key: str, data: Any, metadata: Dict, ttl_seconds: int) -> bool:
        """ä¿å­˜åˆ°Redisç¼“å­˜"""
        redis_client = self.db_manager.get_redis_client()
        if not redis_client:
            return False
        
        try:
            cache_data = {
                'data': data,
                'metadata': metadata,
                'timestamp': datetime.now().isoformat(),
                'backend': 'redis'
            }
            
            serialized_data = pickle.dumps(cache_data)
            redis_client.setex(cache_key, ttl_seconds, serialized_data)
            
            self.logger.debug(f"Redisç¼“å­˜ä¿å­˜æˆåŠŸ: {cache_key}")
            return True
            
        except Exception as e:
            self.logger.error(f"Redisç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def _load_from_redis(self, cache_key: str) -> Optional[Dict]:
        """ä»Redisç¼“å­˜åŠ è½½"""
        redis_client = self.db_manager.get_redis_client()
        if not redis_client:
            return None
        
        try:
            serialized_data = redis_client.get(cache_key)
            if not serialized_data:
                return None
            
            cache_data = pickle.loads(serialized_data)
            
            # è½¬æ¢æ—¶é—´æˆ³
            if isinstance(cache_data['timestamp'], str):
                cache_data['timestamp'] = datetime.fromisoformat(cache_data['timestamp'])
            
            self.logger.debug(f"Redisç¼“å­˜åŠ è½½æˆåŠŸ: {cache_key}")
            return cache_data
            
        except Exception as e:
            self.logger.error(f"Redisç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            return None
    
    def _save_to_mongodb(self, cache_key: str, data: Any, metadata: Dict, ttl_seconds: int) -> bool:
        """ä¿å­˜åˆ°MongoDBç¼“å­˜"""
        mongodb_client = self.db_manager.get_mongodb_client()
        if not mongodb_client:
            return False
        
        try:
            db = mongodb_client.tradingagents
            collection = db.cache
            
            # åºåˆ—åŒ–æ•°æ®
            if isinstance(data, pd.DataFrame):
                serialized_data = data.to_json()
                data_type = 'dataframe'
            else:
                serialized_data = pickle.dumps(data).hex()
                data_type = 'pickle'
            
            cache_doc = {
                '_id': cache_key,
                'data': serialized_data,
                'data_type': data_type,
                'metadata': metadata,
                'timestamp': datetime.now(),
                'expires_at': datetime.now() + timedelta(seconds=ttl_seconds),
                'backend': 'mongodb'
            }
            
            collection.replace_one({'_id': cache_key}, cache_doc, upsert=True)
            
            self.logger.debug(f"MongoDBç¼“å­˜ä¿å­˜æˆåŠŸ: {cache_key}")
            return True
            
        except Exception as e:
            self.logger.error(f"MongoDBç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def _load_from_mongodb(self, cache_key: str) -> Optional[Dict]:
        """ä»MongoDBç¼“å­˜åŠ è½½"""
        mongodb_client = self.db_manager.get_mongodb_client()
        if not mongodb_client:
            return None
        
        try:
            db = mongodb_client.tradingagents
            collection = db.cache
            
            doc = collection.find_one({'_id': cache_key})
            if not doc:
                return None
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if doc.get('expires_at') and doc['expires_at'] < datetime.now():
                collection.delete_one({'_id': cache_key})
                return None
            
            # ååºåˆ—åŒ–æ•°æ®
            if doc['data_type'] == 'dataframe':
                data = pd.read_json(doc['data'])
            else:
                data = pickle.loads(bytes.fromhex(doc['data']))
            
            cache_data = {
                'data': data,
                'metadata': doc['metadata'],
                'timestamp': doc['timestamp'],
                'backend': 'mongodb'
            }
            
            self.logger.debug(f"MongoDBç¼“å­˜åŠ è½½æˆåŠŸ: {cache_key}")
            return cache_data
            
        except Exception as e:
            self.logger.error(f"MongoDBç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            return None
    
    def save_data(self, symbol: str, data: Any, start_date: str = "", end_date: str = "",
                  data_source: str = "default", data_type: str = "stock_data") -> str:
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._get_cache_key(symbol, start_date, end_date, data_source, data_type)

        # å‡†å¤‡å…ƒæ•°æ®
        metadata = {
            'symbol': symbol,
            'start_date': start_date,
            'end_date': end_date,
            'data_source': data_source,
            'data_type': data_type
        }

        # è·å–TTL
        ttl_seconds = self._get_ttl_seconds(symbol, data_type)

        # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šè®°å½•TTLç±»å‹
        self.logger.debug(f"ğŸ’¾ [AdaptiveCache] save_data: symbol={symbol}, data_type={data_type}, "
                          f"ttl_seconds={ttl_seconds} (type={type(ttl_seconds).__name__}), "
                          f"primary_backend={self.primary_backend}")

        # æ ¹æ®ä¸»è¦åç«¯ä¿å­˜
        success = False

        if self.primary_backend == "redis":
            success = self._save_to_redis(cache_key, data, metadata, ttl_seconds)
        elif self.primary_backend == "mongodb":
            success = self._save_to_mongodb(cache_key, data, metadata, ttl_seconds)
        elif self.primary_backend == "file":
            success = self._save_to_file(cache_key, data, metadata)

        # å¦‚æœä¸»è¦åç«¯å¤±è´¥ï¼Œä½¿ç”¨é™çº§ç­–ç•¥
        if not success and self.fallback_enabled:
            self.logger.warning(f"ä¸»è¦åç«¯({self.primary_backend})ä¿å­˜å¤±è´¥ï¼Œä½¿ç”¨æ–‡ä»¶ç¼“å­˜é™çº§")
            success = self._save_to_file(cache_key, data, metadata)

        if success:
            self.logger.info(f"æ•°æ®ç¼“å­˜æˆåŠŸ: {symbol} -> {cache_key} (åç«¯: {self.primary_backend})")
        else:
            self.logger.error(f"æ•°æ®ç¼“å­˜å¤±è´¥: {symbol}")

        return cache_key
    
    def load_data(self, cache_key: str) -> Optional[Any]:
        """ä»ç¼“å­˜åŠ è½½æ•°æ®"""
        cache_data = None
        
        # æ ¹æ®ä¸»è¦åç«¯åŠ è½½
        if self.primary_backend == "redis":
            cache_data = self._load_from_redis(cache_key)
        elif self.primary_backend == "mongodb":
            cache_data = self._load_from_mongodb(cache_key)
        elif self.primary_backend == "file":
            cache_data = self._load_from_file(cache_key)
        
        # å¦‚æœä¸»è¦åç«¯å¤±è´¥ï¼Œå°è¯•é™çº§
        if not cache_data and self.fallback_enabled:
            self.logger.debug(f"ä¸»è¦åç«¯({self.primary_backend})åŠ è½½å¤±è´¥ï¼Œå°è¯•æ–‡ä»¶ç¼“å­˜")
            cache_data = self._load_from_file(cache_key)
        
        if not cache_data:
            return None
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆä»…å¯¹æ–‡ä»¶ç¼“å­˜ï¼Œæ•°æ®åº“ç¼“å­˜æœ‰è‡ªå·±çš„TTLæœºåˆ¶ï¼‰
        if cache_data.get('backend') == 'file':
            symbol = cache_data['metadata'].get('symbol', '')
            data_type = cache_data['metadata'].get('data_type', 'stock_data')
            ttl_seconds = self._get_ttl_seconds(symbol, data_type)
            
            if not self._is_cache_valid(cache_data['timestamp'], ttl_seconds):
                self.logger.debug(f"æ–‡ä»¶ç¼“å­˜å·²è¿‡æœŸ: {cache_key}")
                return None
        
        return cache_data['data']
    
    def find_cached_data(self, symbol: str, start_date: str = "", end_date: str = "", 
                        data_source: str = "default", data_type: str = "stock_data") -> Optional[str]:
        """æŸ¥æ‰¾ç¼“å­˜çš„æ•°æ®"""
        cache_key = self._get_cache_key(symbol, start_date, end_date, data_source, data_type)
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
        if self.load_data(cache_key) is not None:
            return cache_key
        
        return None
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        # æ ‡å‡†ç»Ÿè®¡æ ¼å¼
        stats = {
            'total_files': 0,
            'stock_data_count': 0,
            'news_count': 0,
            'fundamentals_count': 0,
            'total_size': 0,  # å­—èŠ‚
            'total_size_mb': 0,  # MB
            'skipped_count': 0
        }

        # åç«¯ä¿¡æ¯
        backend_info = {
            'primary_backend': self.primary_backend,
            'fallback_enabled': self.fallback_enabled,
            'database_available': self.db_manager.is_database_available(),
            'mongodb_available': self.db_manager.is_mongodb_available(),
            'redis_available': self.db_manager.is_redis_available(),
            'file_cache_directory': str(self.cache_dir),
            'file_cache_count': len(list(self.cache_dir.glob("*.pkl"))),
        }

        total_size_bytes = 0

        # MongoDBç»Ÿè®¡
        mongodb_client = self.db_manager.get_mongodb_client()
        if mongodb_client:
            try:
                db = mongodb_client.tradingagents

                # ç»Ÿè®¡å„ä¸ªé›†åˆ
                for collection_name in ["stock_data", "news_data", "fundamentals_data"]:
                    if collection_name in db.list_collection_names():
                        collection = db[collection_name]
                        count = collection.count_documents({})

                        # è·å–é›†åˆå¤§å°
                        try:
                            coll_stats = db.command("collStats", collection_name)
                            size = coll_stats.get("size", 0)
                            total_size_bytes += size
                        except:
                            pass

                        stats['total_files'] += count

                        # æŒ‰ç±»å‹åˆ†ç±»
                        if collection_name == "stock_data":
                            stats['stock_data_count'] += count
                        elif collection_name == "news_data":
                            stats['news_count'] += count
                        elif collection_name == "fundamentals_data":
                            stats['fundamentals_count'] += count

                backend_info['mongodb_cache_count'] = stats['total_files']
            except:
                backend_info['mongodb_status'] = 'Error'

        # Redisç»Ÿè®¡
        redis_client = self.db_manager.get_redis_client()
        if redis_client:
            try:
                redis_info = redis_client.info()
                backend_info['redis_memory_used'] = redis_info.get('used_memory_human', 'N/A')
                backend_info['redis_keys'] = redis_client.dbsize()
            except:
                backend_info['redis_status'] = 'Error'

        # æ–‡ä»¶ç¼“å­˜ç»Ÿè®¡
        if self.primary_backend == 'file' or self.fallback_enabled:
            for pkl_file in self.cache_dir.glob("*.pkl"):
                try:
                    total_size_bytes += pkl_file.stat().st_size
                except:
                    pass

        # è®¾ç½®æ€»å¤§å°
        stats['total_size'] = total_size_bytes
        stats['total_size_mb'] = round(total_size_bytes / (1024 * 1024), 2)

        # æ·»åŠ åç«¯è¯¦ç»†ä¿¡æ¯
        stats['backend_info'] = backend_info

        return stats
    
    def clear_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        self.logger.info("å¼€å§‹æ¸…ç†è¿‡æœŸç¼“å­˜...")
        
        # æ¸…ç†æ–‡ä»¶ç¼“å­˜
        cleared_files = 0
        for cache_file in self.cache_dir.glob("*.pkl"):
            try:
                with open(cache_file, 'rb') as f:
                    cache_data = pickle.load(f)
                
                symbol = cache_data['metadata'].get('symbol', '')
                data_type = cache_data['metadata'].get('data_type', 'stock_data')
                ttl_seconds = self._get_ttl_seconds(symbol, data_type)
                
                if not self._is_cache_valid(cache_data['timestamp'], ttl_seconds):
                    cache_file.unlink()
                    cleared_files += 1
                    
            except Exception as e:
                self.logger.error(f"æ¸…ç†ç¼“å­˜æ–‡ä»¶å¤±è´¥ {cache_file}: {e}")
        
        self.logger.info(f"æ–‡ä»¶ç¼“å­˜æ¸…ç†å®Œæˆï¼Œåˆ é™¤ {cleared_files} ä¸ªè¿‡æœŸæ–‡ä»¶")
        
        # MongoDBä¼šè‡ªåŠ¨æ¸…ç†è¿‡æœŸæ–‡æ¡£ï¼ˆé€šè¿‡expires_atå­—æ®µï¼‰
        # Redisä¼šè‡ªåŠ¨æ¸…ç†è¿‡æœŸé”®


# å…¨å±€ç¼“å­˜ç³»ç»Ÿå®ä¾‹
_cache_system = None

def get_cache_system() -> AdaptiveCacheSystem:
    """è·å–å…¨å±€è‡ªé€‚åº”ç¼“å­˜ç³»ç»Ÿå®ä¾‹"""
    global _cache_system
    if _cache_system is None:
        _cache_system = AdaptiveCacheSystem()
    return _cache_system
